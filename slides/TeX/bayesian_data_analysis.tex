\documentclass{slides}
\usepackage{xspace,tikz,graphicx,nth,hyperref}
\usepackage{fontawesome}

\title[Bayesian Inference]{Introduction to Bayesian Data Analysis}
\author[Andrews]{Mark Andrews \\ $\phantom{foo}$ \\ Psychology, Nottingham Trent University \\ $\phantom{foo}$ \\ \texttt{mark.andrews@ntu.ac.uk} \\ $\phantom{foo}$ \\ \faTwitter \texttt{@xmjandrews}}

\date{}

\begin{document}
{
	\begin{frame}
		\titlepage
	\end{frame}
}

\begin{frame}
	\frametitle{Introduction}
	\begin{itemize}

	\item Bayesian data analysis is a general approach to statistical data
		analysis that is based on the extensive application of
		probability theory to all problems of inference and prediction.  

	\item It is often contrasted with the more familiar \emph{classical} or
		\emph{frequentist} approach to data analysis.  

	\item While these approaches have co-existed throughout \nth{20}
		century statistics, until recently, the Bayesian approach was
		relatively marginalized.  

	\item Since the early 1990s, however, there has been a remarkable rise
		in the prevalence of Bayesian methods. 

	\item For example, we estimate that approximately 20\% of recently
		published articles in high profile statistics journals are now
		on the topic of Bayesian methods. Likewise, there has been a
		concomitant rise in the prevalence of Bayesian methods in
		psychology. 
\end{itemize}

\end{frame}



\begin{frame}
	\frametitle{The Origin of Bayesian Data Analysis}
	\begin{itemize}

	\item  The origin of Bayesian inference can be traced to a posthumously published 1763 essay
		\emph{Towards Solving a Problem in the Doctrine of Chances},
		written some years earlier by Reverend Thomas Bayes
		(1701-1761). 

	\item In this essay, Bayes focused on a problem that is mathematically
		identical to the problem of inferring the bias of a coin after
		observing the outcome of a sequence of flips of that coin.  
		
	\item Bayes showed that the probability that the coin's bias is exactly
		$\theta$ is proportional to the prior probability of $\theta$
		multiplied by the probability of the observed outcomes given
		$\theta$.
\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Laplace and Inverse Inference}
	\begin{itemize}
		\item Shortly after the publication of Bayes's essay,  the
			French polymath Pierre-Simon Laplace (1749-1827)
			independently rediscovered Bayes' theorem and began to
			apply it to practical problems of data analysis.


	\item Laplace was the first to present Bayesian inference in what is now its modern form:
		\[
			\underbrace{\Prob{\theta \given \data}}_{\text{Posterior}}
			= 
			\frac{\overbrace{\Prob{\data \given \theta}}^{\text{likelihood}}
				\overbrace{\Prob{\theta}}^{\text{prior}}}
				{\underbrace{\int \Prob{\data \given \theta}\Prob{\theta} d\theta}_{\text{marginal likelihood}}}
		\]

\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Frequentism}
	\begin{itemize}

		\item By the early \nth{20} century, a frequentist definition
			of probability became increasingly widely adopted.

		\item Crucially, frequentism entails that the concept of
			probability only applies to the outcomes of a random
			\emph{experiment}.

		\item As such, a parameter such as the bias of a coin, being a
			fixed but unknown quantity, can not be given a
			probabilistic interpretation.  

		\item From this perspective, in the words of R. A. Fisher,
			Bayesian methods were ``\ldots founded upon an error,
			and must be wholly rejected.'' because ``(i)nferences
			respecting populations, from which known samples have
			been drawn, cannot be expressed in terms of
			probability''. Fisher (1925).

	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{The Return of Bayesian Methods}
	
	\begin{itemize}

		\item Bayesian methods remained marginalized from the early to
			the late \nth{20} century

		\item However, their potential practical advantage over
			classical methods was that the could be applied in
			principle to any problem where a probabilistic model of
			data could be specified.  

		\item When computer power was minimal, any ``in principle''
			advantages of Bayesian methods were not important.

		\item However, the roughly exponential growth of computational
			power from the 1970s onwards was the eventual catalyst
			for the adoption of Bayesian methods.

		\item By the late 1980's, Bayesian methods began returning to
			widespread use in science.

	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Inference using Bayes's rule}
	\begin{itemize}

		\item In any statistical model, we assume a probabilistic \emph{generative model} of the data being analyzed.
		\item In other words, in any statistical problem, we have data $\mathcal{D}$ and we assume that this has been drawn from some probability distribution with fixed but unknown parameters $\theta$.
		\item For example, in an independent samples t-test, we assume we have two data set drawn independently from two Normal distributions with fixed but unknown means.
		\item Traditional methods proceed by determing the \emph{maximum likelihood estimator} of $\theta$, denoted $\hat{\theta}$, and using the sampling distribution of $\hat{\theta}$ to test hypotheses using p-values, or to calculate confidence intervals.
		\item Bayesian methods, on the other hand, calculate the probability distribution over all possible values of the the unknown $\theta$ by using Bayes's rule:
		\[
			\underbrace{\Prob{\theta \given \data}}_{\text{Posterior}}
			\propto 
			\underbrace{\mathrm{P}(\data \given \theta)}_{\text{likelihood}}\underbrace{\Prob{\theta}}_{\text{prior}}
		\]
	\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Inference in a Bernoulli Distribution}

\begin{quotation} \ldots Polish mathematicians Tomasz Gliszczynski and
	Waclaw Zawadowski\ldots spun a Belgian one euro coin 250 times,
	and found it landed heads up 140 times \ldots When tossed 250
	times, the one euro coin came up heads 139 times and tails 111.
	\ldots 
\end{quotation}

\flushright{\emph{The Guardian}, January 4, 2002\footnote{ See
	\texttt{http://bit.ly/1BOKu9b} for original story and
	\texttt{http://bit.ly/1BOKx4Q} for discussion.}}

	\begin{itemize}
		\item A sample of $n=250$ coin tosses can be modelled as $n$ independent and identically dsibution Bernoulli random variables with parameter $\theta$.
		\item In other words, our probabilitic model is
			\[
				x_i \sim \textrm{Bernoulli}(\theta),\quad\text{for $i \in \{1,2\ldots n\}$.}
			\]
			and we would like to inference the probable values of $\theta$ given an observation of $m=139$ (or $m=140$, etc.).
	\end{itemize}
\end{frame}
\begin{frame}
	\frametitle{Inference in a Bernoulli Distribution}
	\begin{itemize}
		\item The probabilistic generative model of the Euro coin toss data is as follows:
			\begin{itemize}
				\item The coin's bias corresponds to the fixed but unknown value of the parameter $\theta$ of a Bernoulli random variable.
				\item The observed outcomes $x_1, x_2 \cdots x_n$ are $n$ iid samples from $\textrm{Bernoulli}(\theta)$.
			\end{itemize}
		\item This generative model can be extended by assuming that $\theta$ is itself drawn from a prior distribution $\Prob{\theta}$:
			\begin{align*}
				\theta &\sim \Prob{\theta},\\
				x_i &\sim \textrm{Bernoulli}(\theta),\quad\text{for $i \in \{1,2\ldots n\}$.}
			\end{align*}
		\item In other words, we assume that a value for $\theta$ was randomly drawn from $\Prob{\theta}$ and then $n$ binary variables were sampled from $\textrm{Bernoulli}(\theta)$.
	\end{itemize}
\end{frame}


\begin{frame}
	\frametitle{Inference in a Bernoulli Distribution}
	\begin{itemize}
		\item We aim to calculate
		\[
			\underbrace{\Prob{\theta \given \data}}_{\text{Posterior}}
			= 
			\frac{\overbrace{\Prob{\data \given \theta}}^{\text{likelihood}}
				\overbrace{\Prob{\theta}}^{\text{prior}}}
				{\underbrace{\int \Prob{\data \given \theta}\Prob{\theta} d\theta}_{\text{marginal likelihood}}}
		\]
		where $\theta$ signifies the coin's bias, and $\data$ is the observed coin toss data.

	\item The \emph{likelihood} of $\theta$ gives the probability of the observed outcomes as a function of $\theta$:
			\begin{align*}
				\Prob{x_1, x_2\cdots x_n \given \theta} &= \prod_{i=1}^n \Prob{x_i \given \theta},\\
				& = \prod_{i=1}^n \theta^{x_i} (1-\theta)^{1-x_i},\\
				& = \theta^{m} (1-\theta)^{n-m}.
				\label{eq:bernoulli-likelihood}
			\end{align*}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Inference in a Bernoulli Distribution}
	\input{binomial.likelihood}
		The likelihood function for $n=250$ and $m=139$.\\
		{\small See also \url{https://lawsofthought.shinyapps.io/binomial_likelihood}}

\end{frame}

\begin{frame}
	\frametitle{Conjugate Priors}
	\begin{itemize}
		\item For a given likelihood function, a \emph{conjugate prior} distribution is a prior probability distribution that leads to a posterior distribution of the same parameteric family.
		\item Using conjugate priors allows Bayesian inference and other probabilistic calculations to be performed analytically.
		\item Only a small subset of probabilistic models have conjugate priors.
		\item However, conjugate priors play a vital role in Monte Carlo methods like Gibbs sampling even in complex models.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{The beta distribution}
	\begin{itemize}
		\item For the binomial likelihood function
\[
	\theta^m (1-\theta)^{n-m}	
\]
a conjugate prior is the beta distribution
\[
	\textrm{Beta}(\theta \given \alpha, \beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} \theta^{\alpha-1}(1 - \theta)^{\beta-1}.
\]
\item The \emph{normalizing constant} term is
	\[
		\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} = \frac{1}{B(\alpha,\beta)},
	\]
	where $B(\alpha,\beta)$ is the beta function:
	\[
		B(\alpha,\beta) = \int \theta^\alpha (1-\theta)^{\beta-1}\ d\theta.
	\]
	\end{itemize}
\end{frame}
\begin{frame}
	\frametitle{The beta distribution}
	\input{beta.3.5.distribution}
	The beta distribution with $\alpha=3$ and $\beta=5$.
\end{frame}
\begin{frame}
	\frametitle{Posterior distribution}
	\begin{itemize}
		\item Denoting the observed data by $D = (n, m)$, with the beta prior, the posterior distribution is 

		\begin{align*}
			\Prob{\theta\given D, \alpha,\beta} &= \frac{\Prob{D\given\theta}\Prob{\theta\given\alpha,\beta}}{\int\Prob{D\given\theta}\Prob{\theta\given\alpha,\beta}\ d\theta},\\
			&\propto \overbrace{\theta^{m} (1-\theta)^{n-m}}^{\text{likelihood}} \times \overbrace{\theta^{\alpha-1}(1 - \theta)^{\beta-1}}^{\text{prior}},\\
			&\propto \theta^{m + \alpha-1}(1 - \theta)^{n-m+\beta-1},\\
			&= \textrm{Beta}(m + \alpha, n - m + \beta).
		\end{align*}
		where the normalizing constant is the reciprocal of the beta function 
		\begin{align*}
			\frac{\Gamma(m + \alpha)\Gamma(n - m + \beta)}{\Gamma(n + \alpha+\beta)} &= \int \theta^{\alpha + m - 1} (1-\theta)^{ \beta + n - m - 1}\ d\theta.\\
			&=B(\alpha + m, \beta + n - m).
		\end{align*}

	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Posterior distribution}
	\begin{itemize}
		\item For our Euro coin example, our observed data are $n=250$ and $m=139$.
		\item A noninformative uniform prior on $\theta$ is $\textrm{Beta}(\alpha=1,\beta=1)$.
		\item With this prior, the posterior distribution is 
			\begin{align*}
				\textrm{Beta}(m + \alpha, n - m + \beta) &= \textrm{Beta}(139 + 1, 250 - 139 + 1),\\
				&= \textrm{Beta}(140, 112)
			\end{align*}
	\end{itemize}
\end{frame}
\begin{frame}
	\frametitle{Posterior distribution}
	\input{beta.140.112.distribution}
	The posterior distribution when $n=250$, $m=139$, $\alpha=1$ and $\beta=1$.\\
	{\small	\url{https://lawsofthought.shinyapps.io/bayesian_coin_inference/}}
\end{frame}

\begin{frame}
	\frametitle{Summarizing the posterior distribution}
	\begin{itemize}
		\item The mean, variance and modes of any beta distribution are as follows:
			\begin{align*}
				\langle \theta \rangle &=\frac{\alpha}{\alpha+\beta} ,\\
				\mathrm{V}(\theta) &= \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)},\\
				\textrm{mode}(\theta) &= \frac{\alpha-1}{\alpha+\beta-2}.
			\end{align*}
		\item Thus, in our case of $\textrm{Beta}(140, 112)$, we have
			\begin{align*}
				\langle \theta \rangle &=  0.5556,\\
				\mathrm{V}(\theta) &= 0.001, \quad\textrm{sd}(\theta) = 0.0312,\\
				\textrm{mode}(\theta) &= 0.556.
			\end{align*}

	\end{itemize}
\end{frame}


\begin{frame}
	\frametitle{High posterior density (\hpd) intervals}
	\begin{itemize}
		\item \hpd intervals provide ranges that contain specified probability mass. For example, the 0.95 \hpd interval is the range of values that contain 0.95 of the probability mass of the distribution.
		\item The $\varphi$ \hpd interval for the probability density function $\Prob{x}$ is computed by finding a probability density value $p^*$ such that 
		\[
			\Prob{\{x \colon \Prob{x} \geq p^*\}} = \varphi.
		\]
		\item In other words, we find the value $p^*$ such that the probability mass of the set of points whose density is greater than than $p^*$ is exactly $\varphi$. 
		\item In general, the \hpd is not trivial to compute but in the case of symmetric distributions, it can be easily computed from the cumulative density function.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{The 0.95 \hpd interval}
	\input{beta.140.112.hpd}
	The posterior distribution, with its $0.95$ \hpd, when $n=250$, $m=139$, $\alpha=1$ and $\beta=1$. In this case, the \hpd interval is $(0.494, 0.617)$.
\end{frame}

\begin{frame}
	\frametitle{Posterior predictive distribution}

	\begin{itemize}
		\item Given that we have observed $m$ heads in $n$ coin tosses, what is the probability that the \emph{next} coin toss is heads.
		\item This is given by the \emph{posterior predictive} probability that $x=1$:
			\begin{align*}
				\Prob{x=1\given D, \alpha, \beta} &= \int \Prob{x=1 \given \theta} \overbrace{\Prob{\theta\given D, \alpha, \beta}}^{\text{Posterior}}\ d\theta,\\
				&= \int \theta \times \Prob{\theta\given D, \alpha, \beta} \ d\theta,\\
				&= \left\langle \theta \right\rangle,\\
				&= \frac{\alpha+m}{\alpha+\beta+n}.
			\end{align*}
		\item Thus, given 139 heads in 250 tosses, the predicted probability that the next coin will also be heads is $\approx 0.5556$.
	\end{itemize}
\end{frame}
	
\begin{frame}
	\frametitle{The role of Markov Chain Monte Carlo}
	
	\begin{itemize}

		\item In general, given observed data $D$ and a model $\Omega$, the posterior distribution over the parameters $\theta$ of the model is
		\[
			\Prob{\theta\given D}
			= \frac{\overbrace{\Prob{D\given\theta}}^{\text{Likelihood}}\overbrace{\Prob{\theta}}^{\text{Prior}}}
			{\underbrace{\int\Prob{D\given\theta}\Prob{\theta}\ d\theta}_{\text{Marginal likelihood}}}.
		\]
		where the \emph{marginal likelihood} gives the likelihood of the model given the observed data.

	\item Given the posterior distribution $\Prob{\theta\given D}$, our aim is often to characterise this distribution in terms of e.g. its mean, variance, etc. 
	\item Likewise, we may aim to calculate \emph{posterior predictive} distributions such as 
		\[
			\Prob{x_{\text{new}} \given D} = \int\Prob{x_{\text{new}}\given\theta}\Prob{\theta\given D}\ d\theta.
		\]

	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Sampling from posterior distributions}
	\begin{itemize}

		\item In only rare situations can we determine the characteristics of the posterior distribution, or calculate posterior predictive distributions, in closed form.
		\item However, in general, if we can draw samples from $\Prob{\theta\given D}$ then we can approximate, e.g., the mean of the distribution by
			\[
				\langle\theta\rangle = \int\theta\Prob{\theta\given D} \approx \frac{1}{N} \sum_{i=1}^N \tilde{\theta}_i,
			\]
		or the posterior predictive distribution by 
		\[
			\Prob{x_{\text{new}} \given D} = \int\Prob{x_{\text{new}}\given\theta}\Prob{\theta\given D}\ d\theta
			\approx \frac{1}{N} \sum_{i=1}^N \Prob{x_{\text{new}}\given\tilde{\theta}_i},
		\]
		where \[\tilde{\theta}_1, \tilde{\theta}_2 \ldots \tilde{\theta}_N\] are samples from $\Prob{\theta\given D}$.
	\end{itemize}
\end{frame}
\begin{frame}
	\frametitle{The Advantages of Bayesian Methods}
	\begin{itemize}	
		\item Bayesian methods provide full probabilistic interpretations of all inferences
and predictions that form the statistical analysis. 
\item This allows for a more meaningful and concise interpretation of testing or estimation procedures.  
\item For example, Bayesian hypothesis
tests allow us explicitly state the probability, contingent on the modelling
assumptions, of one hypothesis relative to another in light of observed data.
Likewise, Bayesian \textsc{hpd}'s allow us to explicitly state the probability
that an unknown quantity's value lies within a certain range of values. 
\item These contrast with classical p-values and
confidence intervals.  
\item For example, a p-value for a hypothesis test gives the
probability a statistic as or more extreme than that obtained could have
happened were the hypothesis true. The meaningfulness of such a statement is
less clear and p-values are
routinely misinterpreted. 
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{The Advantages of Bayesian Methods}

	\begin{itemize}	
		
		
	\item  In any probabilistic model that can be formally specified,
		inference concerning any variable is always possible in
		principle. 

	\item In other words, the Bayesian approach provides a universal method
		of inference. The probable values of any variables in the model
		--- partially observed variables, latent variables, parameters,
		hyper-parameters, etc. --- can always be inferred in principle,
		once the model has been defined. 

	\item This contrasts starkly with classical methods where, beyond a
		small set of special cases, inference proceeds in a case by
		case manner, often with each problem requiring its own solution
		and even simple problems being surprisingly intractable.  

	\item The practical consequence of this is that Bayesian methods will
		be applicable to arbitrary probabilistic models, not simply
		those with convenient properties or those that have been
		studied intensely and solved.  

	\item This allows the scientist or researcher to choose or develop a
		model based on its scientific meaning rather then on
		mathematical convenience. 

\end{itemize}
\end{frame}

\end{document}
